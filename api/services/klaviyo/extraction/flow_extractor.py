"""
Flow data extraction module.
"""
import asyncio
import logging
from typing import Dict, Any, List

logger = logging.getLogger(__name__)


class FlowExtractor:
    """Extracts flow data from Klaviyo."""
    
    def __init__(self, flows, flow_stats):
        """
        Initialize flow extractor.
        
        Args:
            flows: FlowsService instance
            flow_stats: FlowStatisticsService instance
        """
        self.flows = flows
        self.flow_stats = flow_stats
    
    async def extract(
        self,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        Extract flow data, statistics, and detailed flow information.
        
        Args:
            verbose: Whether to print progress messages
            
        Returns:
            Dict with 'flows', 'flow_statistics', and 'flow_details' keys
        """
        if verbose:
            print("\nðŸ”„ SECTION 3: Flow Data")
            print("-" * 40)
        
        flows = await self.flows.get_flows()
        if verbose:
            print(f"  âœ“ Fetched {len(flows)} flows")
        
        flow_statistics = {}
        if flows:
            # Batch flows to reduce API load - process in smaller chunks
            all_flow_ids = [f["id"] for f in flows[:50]]
            batch_size = 5  # Reduced to 5 flows per batch to avoid rate limiting
            
            if verbose:
                print(f"  Fetching statistics for {len(all_flow_ids)} flows in batches of {batch_size}...")
            
            # Resolve conversion_metric_id once before processing batches
            # This prevents repeated lookups and warnings
            conversion_metric_id = None
            if hasattr(self.flow_stats, '_cached_conversion_metric_id') and self.flow_stats._cached_conversion_metric_id:
                conversion_metric_id = self.flow_stats._cached_conversion_metric_id
            
            # Process flows in batches with delays
            for i in range(0, len(all_flow_ids), batch_size):
                batch_ids = all_flow_ids[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (len(all_flow_ids) + batch_size - 1) // batch_size
                
                if verbose and total_batches > 1:
                    print(f"    Processing batch {batch_num}/{total_batches} ({len(batch_ids)} flows)...")
                
                try:
                    # Use cached conversion_metric_id if available (after first batch)
                    if conversion_metric_id is None and hasattr(self.flow_stats, '_cached_conversion_metric_id'):
                        conversion_metric_id = self.flow_stats._cached_conversion_metric_id
                    
                    batch_stats = await self.flow_stats.get_statistics(
                        flow_ids=batch_ids,
                        timeframe="last_365_days",
                        conversion_metric_id=conversion_metric_id  # Pass it explicitly to avoid repeated lookups
                    )
                    
                    # After first batch, get the cached value for subsequent batches
                    if conversion_metric_id is None and hasattr(self.flow_stats, '_cached_conversion_metric_id'):
                        conversion_metric_id = self.flow_stats._cached_conversion_metric_id
                    
                    # Merge batch results
                    if batch_stats:
                        if not flow_statistics:
                            flow_statistics = batch_stats
                        else:
                            # Merge results if multiple batches
                            batch_results = batch_stats.get("data", {}).get("attributes", {}).get("results", [])
                            if batch_results and "data" in flow_statistics:
                                existing_results = flow_statistics["data"]["attributes"].get("results", [])
                                existing_results.extend(batch_results)
                    
                    # Add delay between batches to avoid rate limiting
                    if i + batch_size < len(all_flow_ids):
                        await asyncio.sleep(8.0)  # 8 second delay between batches (increased to prevent rate limiting)
                        
                except Exception as e:
                    if verbose:
                        print(f"    âš ï¸ Batch {batch_num} failed: {e}")
                    # Add extra delay after failure before retrying next batch
                    if i + batch_size < len(all_flow_ids):
                        await asyncio.sleep(10.0)  # Extra delay after failure
                    continue
            
            if flow_statistics and verbose:
                total_results = 0
                if "data" in flow_statistics and "attributes" in flow_statistics["data"]:
                    total_results = len(flow_statistics["data"]["attributes"].get("results", []))
                print(f"  âœ“ Flow statistics extracted for {total_results} flows")
        
        # Detailed flow data
        if verbose:
            print(f"  Fetching detailed data for first 5 flows...")
        flow_data = []
        for i, flow in enumerate(flows[:5]):
            try:
                actions = await self.flows.get_flow_actions(flow["id"])
                await asyncio.sleep(0.5)
                
                limited_actions = actions[:3]
                flow_messages = []
                
                for action in limited_actions:
                    try:
                        messages = await self.flows.get_flow_action_messages(action["id"])
                        flow_messages.extend(messages)
                        await asyncio.sleep(0.5)
                    except Exception as e:
                        if verbose:
                            print(f"    âœ— Error fetching messages for action {action['id']}: {e}")
                        break
                
                flow_data.append({
                    "flow": flow,
                    "actions": limited_actions,
                    "messages": flow_messages
                })
                if verbose:
                    print(f"    âœ“ Flow {i+1}/5: {len(limited_actions)} actions, {len(flow_messages)} messages")
            except Exception as e:
                if verbose:
                    print(f"    âœ— Error fetching data for flow {flow['id']}: {e}")
        
        return {
            "flows": flows,
            "flow_statistics": flow_statistics,
            "flow_details": flow_data
        }

